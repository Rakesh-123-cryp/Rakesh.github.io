<!DOCTYPE html>
<html>
<head>
  <!-- <title>Nerfies: Deformable Neural Radiance Fields</title> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="icon" href="./images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MedGRIM: Enhanced Zero-shot Medical VQA using prompt-embedded Multimodal GraphRAG</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rakesh-123-cryp.github.io/Rakesh.github.io/">Rakesh Raj</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="">Akshat Kailmal</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="">Hashim Faisal</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nLYLiE8AAAAJ&hl=en">Chandrakala.S</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shiv Nadar University Chennai</span>
          </div>

          <!-- <div class="text"></div>
            <p>Under review in WACV 2025,</p>
            <p>All the links will be activated on acceptence of the paper</p>
          </div> -->
          <h2 class="subtitle has-text-centered">
            <b style="color:tomato;">Submitted to ICCV 2025<br>Links will be updated on acceptance</b><br><br>
          </h2>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <!-- <b style="color:tomato;">!! The links will be updated after the acceptence of the paper !!</b><br><br> -->
        <span class="dnerf">MedGRIM</span> is an interactive medical AI Agent with transparent thought process<br>
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <center><img src="./assets/Video_Results.gif" alt="Sample GIFs" style="width:1000px;height:700px;"></center> -->
<!-- <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
  <source src="./assests/Video_Results.mp4"
          type="video/mp4">
</video> -->
<!-- <center>
<video width="1000px" 
           height="900px" 
           controls="controls" autoplay loop>
        <source src="./assets/Video_Results.mp4"
                type="video/mp4" />
</video>
</center> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          An ensemble of trained multimodal encoders and Vision-Language Models (VLMs) has become a standard approach for 
          Visual Question Answering (VQA) tasks. However, such naive models often fail to produce responses with the detailed 
          precision necessary for complex, domain-specific applications such as medical VQA. Our representation model, 
          <b>BIND</b>: <b>B</b>LIVA <b>In</b>tegrated with <b>D</b>ense Encoding, extends prior multimodal work by 
          refining the joint embedding space through dense, query-token-based encodings, inspired by contrastive pretraining techniques. 
          This refined encoder powers <b>Med-GRIM</b>, a model designed for medical VQA tasks that leverages graph-based retrieval 
          and prompt engineering to integrate domain-specific knowledge. Rather than relying on compute-heavy fine-tuning of vision 
          and language models on specific datasets, Med-GRIM applies a low-compute, modular workflow with small language models (SLMs) 
          for efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject relevant knowledge, ensuring both accuracy and 
          robustness in its responses. By assigning distinct roles to each agent within the VQA system, Med-GRIM achieves large 
          language model performance at a fraction of the computational cost. Additionally, to support scalable research in zero-shot 
          multimodal medical applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising diverse dermatological 
          conditions. This dataset facilitates both multimodal and unimodal querying. The code and dataset will be released on acceptance.
          </p>
          <br>
          <h2 class="title is-3"><center>Contributions</center></h2>
          <ul>
            <li>We propose a novel multimodal representation learning architecture that advances performance on state-of-the-art datasets in multimodal learning.</li>
            <li>OWe introduce DermaGraph, a multimodal, graph-structured dataset designed for RAG tasks in dermatology, which also supports unimodal input scenarios.</li>
            <li>We develop a zero-shot learning pipeline specifically tailored for medical query processing, enabling accurate and efficient responses to medical inquiries.</li>
            <li>We design a graph filtering mechanism for improved diagnostic accuracy in medical applications.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<center><img src="./assets/main_arch.png" alt="Model Architecture" style="width:1000px;height:400px;"></center>
<br>

<section class="section"></section>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><b>MedGRIM: Method</b></h2>
        <div class="content has-text-justified">
          <p>
            Med-GRIM integrates multimodal inputs—such as images and descriptions—through a series of specialized 
            modules, including BIND, graph retrieval layers and prompt injection . The model first assesses possible 
            conditions, ranking them by probability, then dynamically retrieves relevant data and refines responses 
            iteratively. This approach allows it to present condition-agnostic insights and tailor responses based 
            on user feedback. Through iterative filtering, Med-GRIM engages users with clarifying questions, adapting 
            its answers based on specific input cues, as shown in the step-by-step reasoning for diagnosing conditions.
          </p>
        </div>
        <br>

        <!-- <h2 class="title is-3"><b>Coarse Estimation</b></h2>
        <div class="content has-text-justified">
          <p>
            Given the global multiplexing of lensless captures, we cannot directly feed them into the radiance 
            fields model to render novel views. Hence, to reconstruct the RGB image from the lensless captures, 
            these need to be deconvolved with the lensless camera's point spread function (PSF) to obtain coarse 
            reconstructed images. For this, we utilize wiener deconvolution, which accepts the lensless capture 
            and the point spread function as the input and returns the reconstructed image.
          </p>
        </div> -->
      </div>
    </div>
  </div>
</section>

<br><br>
<div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3"><b>Pompt Templates Used</b></h2>
      <div class="content has-text-justified">
        <p>It emphasizes structuring and framing instructional prompts to align closely with the patterns and context the model was trained on. Leveraging this approach, we employ a custom-designed prompt template tailored specifically to our application.</p>
        <p>Some essential considerations when creating an effective prompt template include:</p>
        <ul>
            <li><strong>Be as specific as possible:</strong> Providing clear, specific prompts reduces ambiguity, enabling the AI to better understand the query and its context.</li>
            <li><strong>Specify the level of detail required:</strong> Indicate preferences for the response format, such as using bullet points, specifying the number of questions, or defining paragraph limits.</li>
            <li><strong>Offer positive guidance:</strong> Focus on explaining what the LLM should do, rather than what it should not do, to maintain clarity and direction.</li>
        </ul>
      </div>
    </div>
  </div>
</div>
<br>
<center>
  <img src="./assets/prompt.png" alt="Scene" style="width:500px;height:500px;">
  <img src="./assets/prompt_template.png" alt="Scene" style="width:500px;height:500px;">
</center>
<br>
<center>
  <h2 class="title is-3"><b>Demo Video</b></h2>
  <video width="1000px" 
             height="900px" 
             controls="controls" autoplay loop>
          <source src="./assets/media1.mp4"
                  type="video/mp4" />
  </video>
<br>
<!-- <center></center> -->
<br>
<div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3"><b>Dataset Overview</b></h2>
      <div class="content has-text-justified">
        <p>These connections illustrate the semantic relationships across nodes, linking dermatological conditions
          with overlapping attributes, such as shared symptoms, similar underlying causes, or common treatment strategies.
           For instance, conditions with symptoms like redness or inflammation may be grouped together, creating meaningful
            clusters that aid in understanding correlations between different conditions. Similarly, treatment strategies
             that apply to multiple conditions, such as the use of topical steroids or antihistamines, establish additional
              connections between nodes. This interconnected design transforms the dataset into a dynamic knowledge graph,
               where the relationships between nodes provide rich context for various tasks.</p>
      </div>
    </div>
  </div>
</div>
<br><br>

<center><img src="./assets/multiple.png" alt="Scene" style="width:700px;height:500px;"></center><br>
<center><img src="./assets/content_dataset.jpeg" alt="Scene" style="width:900px;height:300px;"></center>
<br>
<br>
<center><h2 class="title is-3"><b>Qualitative Result</b></h2></center>
<br>
<center><img src="./assets/qualitative.png" alt="Scene" style="width:1200px;height:400px;"></center>


<!-- <section class="section">
  <div class="container is-max-desktop"> -->

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  <!-- </div>
</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Raj, Rakesh and Kaimal, Akshat and K.V. , Badhrinarayanan and Gupta , Vinayak and Rohit, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {GANESH: Generalizable NeRF for Lensless Imaging},
  journal   = {Submitted to WACV},
  year      = {2025},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
